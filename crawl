#!/usr/bin/env python

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import json
import sys
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed

def get_domain(url):
    """Extracts the (scheme, netloc) part of a URL to identify its domain."""
    parsed = urlparse(url)
    return parsed.scheme, parsed.netloc

def normalize_url(url):
    """Normalize URL by removing fragment identifier."""
    parsed = urlparse(url)
    return urljoin(url, parsed.path + ('?' + parsed.query if parsed.query else ''))

def is_same_domain(url, base_domain):
    """Checks whether 'url' is in the same domain as 'base_domain'."""
    scheme, netloc = get_domain(url)
    return (scheme, netloc) == base_domain

def process_url(url_info):
    """Process a single URL and return its data."""
    url, base_domain, depth = url_info
    result = {"url": url, "links": []}

    try:
        response = requests.get(url, timeout=10)
        status_code = response.status_code

        if status_code == 404:
            print(f"Found 404: {url}", file=sys.stderr)
            return url, {"status_code": 404, "size": 0, "links": []}

        page_content = response.content
        size_in_bytes = len(page_content)
        result.update({"status_code": status_code, "size": size_in_bytes})

        # Only proceed if it's HTML
        content_type = response.headers.get("Content-Type", "")
        if "text/html" in content_type.lower():
            soup = BeautifulSoup(page_content, "html.parser")
            for link_tag in soup.find_all("a", href=True):
                raw_link = link_tag.get("href")
                next_link = urljoin(url, raw_link)
                normalized_link = normalize_url(next_link)

                # For internal links - continue crawling with no depth limit
                if is_same_domain(normalized_link, base_domain):
                    result["links"].append((normalized_link, depth))  # Keep same depth for internal links
                # For external links - only check them once (depth 1)
                elif depth == 0:  # Only add external links from internal pages
                    result["links"].append((normalized_link, 1))  # External links always get depth 1

            print(f"Found {len(result['links'])} new links from {url}", file=sys.stderr)

    except requests.RequestException as e:
        print(f"Error fetching {url}: {e}", file=sys.stderr)
        return url, {"status_code": "error", "size": 0, "links": []}

    return url, result

def crawl_site(start_url, max_workers=10):
    print(f"Starting crawl from {start_url}", file=sys.stderr)
    if not start_url.startswith(('http://', 'https://')):
        start_url = 'http://' + start_url

    base_domain = get_domain(start_url)
    print(f"Base domain: {base_domain[0]}://{base_domain[1]}", file=sys.stderr)

    # Track URLs to process and those we've seen
    normalized_start = normalize_url(start_url)
    to_process = [(normalized_start, base_domain, 0)]
    seen_urls = {normalized_start}
    results = {}

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        while to_process:
            # Process URLs in parallel batches
            future_to_url = {
                executor.submit(process_url, url_info): url_info[0]
                for url_info in to_process
            }

            to_process = []

            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    url, data = future.result()
                    results[url] = data

                    # Add new links to process
                    for new_url, new_depth in data.get("links", []):
                        if new_url not in seen_urls:
                            seen_urls.add(new_url)
                            to_process.append((new_url, base_domain, new_depth))

                except Exception as e:
                    print(f"Error processing {url}: {e}", file=sys.stderr)
                    results[url] = {"status_code": "error", "size": 0}

    print(f"Crawl complete. Visited {len(results)} URLs", file=sys.stderr)
    return results

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Crawl a website and check for broken links and size limits')
    parser.add_argument('url', help='The URL to start crawling from')
    parser.add_argument('--max-size', type=int, help='Maximum allowed size in bytes for any page', default=None)
    parser.add_argument('--workers', type=int, help='Number of parallel workers', default=10)
    args = parser.parse_args()

    report = crawl_site(args.url, max_workers=args.workers)

    has_errors = False
    size_exceeded = False
    # Output in JSONL format
    for url, info in report.items():
        result = {
            "url": url,
            "status_code": info["status_code"],
            "size": info["size"]
        }
        print(json.dumps(result))
        if info["status_code"] == 404:
            has_errors = True
        if args.max_size and info["size"] > args.max_size:
            print(f"Error: {url} exceeds maximum size of {args.max_size} bytes (actual size: {info['size']} bytes)", file=sys.stderr)
            size_exceeded = True
    
    if has_errors or size_exceeded:
        sys.exit(1)
