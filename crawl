#!/usr/bin/env python

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import json
import sys
import logging
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed


def setup_logging(debug=False):
    """Configure logging with two handlers: one for stdout (INFO) and one for stderr (ERROR and DEBUG)"""
    # Create our own logger
    logger = logging.getLogger('crawler')
    logger.handlers = []  # Clear any existing handlers
    logger.propagate = False  # Don't propagate to root logger

    # Set base level based on debug flag
    logger.setLevel(logging.DEBUG if debug else logging.INFO)

    # Create formatter
    formatter = logging.Formatter('%(message)s')

    # Create stdout handler for INFO level (JSONL output)
    stdout_handler = logging.StreamHandler(sys.stdout)
    stdout_handler.setLevel(logging.INFO)
    stdout_handler.setFormatter(formatter)
    stdout_handler.addFilter(lambda record: record.levelno == logging.INFO)

    # Create stderr handler for ERROR and DEBUG
    stderr_handler = logging.StreamHandler(sys.stderr)
    stderr_handler.setLevel(logging.DEBUG)
    stderr_handler.setFormatter(formatter)
    stderr_handler.addFilter(lambda record: record.levelno != logging.INFO)

    # Add handlers to our logger
    logger.addHandler(stdout_handler)
    logger.addHandler(stderr_handler)

    return logger


def get_domain(url):
    """Extracts the (scheme, netloc) part of a URL to identify its domain."""
    parsed = urlparse(url)
    return parsed.scheme, parsed.netloc


def normalize_url(url):
    """Normalize URL by removing fragment identifier."""
    parsed = urlparse(url)
    return urljoin(url, parsed.path + ('?' + parsed.query if parsed.query else ''))


def is_same_domain(url, base_domain):
    """Checks whether 'url' is in the same domain as 'base_domain'."""
    scheme, netloc = get_domain(url)
    return (scheme, netloc) == base_domain


def process_url(url_info):
    """Process a single URL and return its data."""
    url, base_domain, depth, referrer = url_info
    result = {"url": url, "links": [], "referrer": referrer}
    logger = logging.getLogger('crawler')

    try:
        headers = {
            'User-Agent': 'curl/8.7.1',
            'Accept': '*/*'
        }
        response = requests.get(
            url, timeout=10, allow_redirects=True, headers=headers)
        status_code = response.status_code

        if 400 <= status_code < 600:
            logger.error(f"Found error status code {status_code}: {url}")
            return url, {"status_code": status_code, "size": 0, "links": [], "referrer": None}

        page_content = response.content
        size_in_bytes = len(page_content)
        result.update({"status_code": status_code, "size": size_in_bytes})

        # Only proceed if it's HTML
        content_type = response.headers.get("Content-Type", "")
        if "text/html" in content_type.lower():
            soup = BeautifulSoup(page_content, "html.parser")
            for link_tag in soup.find_all("a", href=True):
                raw_link = link_tag.get("href")
                next_link = urljoin(url, raw_link)
                normalized_link = normalize_url(next_link)

                # For internal links - continue crawling with no depth limit
                if is_same_domain(normalized_link, base_domain):
                    # Keep same depth for internal links
                    result["links"].append((normalized_link, depth, url))
                # For external links - only check them once (depth 1)
                elif depth == 0:  # Only add external links from internal pages
                    # External links always get depth 1
                    result["links"].append((normalized_link, 1, url))

            logger.debug(f"Found {len(result['links'])} new links from {url}")

    except requests.RequestException as e:
        logger.error(f"Error fetching {url}: {e}")
        return url, {"status_code": "error", "size": 0, "links": [], "referrer": None}

    return url, result


def crawl_site(start_url, max_workers=10):
    logger = logging.getLogger('crawler')
    logger.debug(f"Starting crawl from {start_url}")
    if not start_url.startswith(('http://', 'https://')):
        start_url = 'https://' + start_url

    base_domain = get_domain(start_url)
    logger.debug(f"Base domain: {base_domain[0]}://{base_domain[1]}")

    # Track URLs to process and those we've seen
    normalized_start = normalize_url(start_url)
    # None referrer for start URL
    to_process = [(normalized_start, base_domain, 0, None)]
    seen_urls = {normalized_start}
    results = {}

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        while to_process:
            # Process URLs in parallel batches
            future_to_url = {
                executor.submit(process_url, url_info): url_info[0]
                for url_info in to_process
            }

            to_process = []

            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    url, data = future.result()
                    results[url] = data

                    # Add new links to process
                    for new_url, new_depth, referrer in data.get("links", []):
                        if new_url not in seen_urls:
                            seen_urls.add(new_url)
                            to_process.append(
                                (new_url, base_domain, new_depth, referrer))

                except Exception as e:
                    logger.error(f"Error processing {url}: {e}")
                    results[url] = {"status_code": "error",
                                    "size": 0, "referrer": None}

    logger.debug(f"Crawl complete. Visited {len(results)} URLs")
    return results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='Crawl a website and check for broken links and size limits')
    parser.add_argument('url', help='The URL to start crawling from')
    parser.add_argument('--max-size', type=int,
                        help='Maximum allowed size in bytes for any page', default=None)
    parser.add_argument('--workers', type=int,
                        help='Number of parallel workers', default=10)
    parser.add_argument('--verbose', action='store_true',
                        help='Enable verbose logging')
    args = parser.parse_args()

    # Setup logging based on debug flag
    logger = setup_logging(args.verbose)

    report = crawl_site(args.url, max_workers=args.workers)

    has_errors = False
    size_exceeded = False
    final_errors = []
    # Sort URLs by size before output
    sorted_items = sorted(report.items(), key=lambda x: x[1]["size"])
    # Output in JSONL format
    for url, info in sorted_items:
        result = {
            "url": url,
            "status_code": info["status_code"],
            "size": info["size"],
            "referrer": info.get("referrer")
        }
        logger.info(json.dumps(result))
        if isinstance(info["status_code"], int) and 400 <= info["status_code"] < 600:
            final_errors.append(
                f"Error: {url} returned status code {info['status_code']}")
            has_errors = True
        if args.max_size and info["size"] > args.max_size:
            final_errors.append(
                f"Error: {url} exceeds maximum size of {args.max_size} bytes (actual size: {info['size']} bytes)")
            size_exceeded = True

    # Print all collected errors at the end
    if final_errors:
        for error in final_errors:
            logger.error(error)

    if has_errors or size_exceeded:
        sys.exit(1)
